{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emilien BirÃ©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "import text_flappy_bird_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_function(Q_dict):\n",
    "    \"\"\"A function to plot a Q function\n",
    "    \"\"\"\n",
    "    x = [k[0] for k in Q_dict.keys()]\n",
    "    y = [k[1] for k in Q_dict.keys()]\n",
    "    q_values = np.array([np.max(v) for v in Q_dict.values()])\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Create a surface plot\n",
    "    ax.plot_trisurf(x, y, q_values, cmap='viridis', edgecolor='none')\n",
    "\n",
    "    # Labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Q-values')\n",
    "    ax.set_title('Q-value Surface')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "obs,_ = env.reset()\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC control agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took the liberty to copy the code from TP4, since it's the same framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state,_ = env.reset()\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        # take a step in the environement \n",
    "        next_state, reward, done, info,_ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * (epsilon / nA)\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    rewards = np.array(rewards)\n",
    "    for i, state in enumerate(states):\n",
    "        g = np.array(rewards)[i:,None].T.dot(discounts[:-i-1,None])[0][0]\n",
    "        old_Q = Q[state][actions[i]]\n",
    "        Q[state][actions[i]] = old_Q + alpha * (g - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MCC(env, Q):\n",
    "    \"\"\"A function that evaluate a trained MCC agent\n",
    "    \"\"\"\n",
    "    tot_rewards = []\n",
    "    nA = env.action_space.n\n",
    "    for i_episode in range(500):\n",
    "        state,_ = env.reset()\n",
    "        r = 0\n",
    "        while True:\n",
    "            action = np.random.choice(np.arange(nA), p=get_probs(Q[state], 0, nA)) \\\n",
    "                                if state in Q else env.action_space.sample()\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            r+=reward\n",
    "            # If player is dead break\n",
    "            if done or r>500: #We set a max reward \n",
    "                tot_rewards.append(r)\n",
    "                r=0\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return tot_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0, epsilon = 0.1, epsilon_end=0.01, plot = False):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    performances = {}\n",
    "    # loop over episodes\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        # cur_eps = epsilon_end * (i_episode/num_episodes) + (1 - (i_episode/num_episodes)) * epsilon\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}. Eps={}\".format(i_episode, num_episodes,epsilon), end=\"\")\n",
    "            policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "            tot_rewards = evaluate_MCC(env, Q)\n",
    "            \n",
    "            performances[str(i_episode)] = tot_rewards\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        if plot and i_episode in [1000, num_episodes//2, num_episodes]:\n",
    "            plot_q_function(Q)\n",
    "        # set the value of epsilon\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        \n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(env,episode, Q, alpha, gamma)\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q, performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating alpha (step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [0.001,0.003,0.005,0.01]:\n",
    "    print(\"Alpha=\",a)\n",
    "    policy, Q, training_perfs = mc_control(env, 30000, a, epsilon=0.2)\n",
    "    training_steps = [int(k) for k in training_perfs.keys()]\n",
    "    mean_rewards = []\n",
    "    std_rewards = []\n",
    "    for rewards in training_perfs.values():\n",
    "        mean_rewards.append(np.array(rewards).mean())\n",
    "        std_rewards.append(np.array(rewards).std())\n",
    "\n",
    "    plt.plot(training_steps, mean_rewards, label=\"Alpha=\"+str(a))\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [0.01, 0.05,0.1,0.2]:\n",
    "    print(\"Eps=\",eps)\n",
    "    policy, Q, training_perfs = mc_control(env, 30000, alpha=0.004, epsilon = eps)\n",
    "    training_steps = [int(k) for k in training_perfs.keys()]\n",
    "    mean_rewards = []\n",
    "    std_rewards = []\n",
    "    for rewards in training_perfs.values():\n",
    "        mean_rewards.append(np.array(rewards).mean())\n",
    "        std_rewards.append(np.array(rewards).std())\n",
    "\n",
    "    plt.plot(training_steps, mean_rewards, label=\"Eps=\"+str(eps))\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mcc_policy, Q, mcc_best_performances = mc_control(env, 30000, alpha = 0.05, epsilon = 0.05, plot = True)\n",
    "training_steps = [int(k) for k in mcc_best_performances.keys()]\n",
    "mean_rewards = []\n",
    "std_rewards = []\n",
    "for rewards in mcc_best_performances.values():\n",
    "    mean_rewards.append(np.array(rewards).mean())\n",
    "    std_rewards.append(np.array(rewards).std())\n",
    "\n",
    "plt.plot(training_steps, mean_rewards)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SARSA(env, agent):\n",
    "    \"\"\"A function to evaluate a trained SARSA agent\n",
    "    \"\"\"\n",
    "    tot_rewards = []\n",
    "\n",
    "    for i_episode in range(500):\n",
    "        state,_ = env.reset()\n",
    "        first_action = agent.agent_start(state)\n",
    "        state, reward, done, _, info = env.step(first_action)\n",
    "        r = reward\n",
    "        while True:\n",
    "            next_action = agent.agent_step(reward,state, training = False)\n",
    "            state, reward, done, _, info = env.step(next_action)\n",
    "            r+=reward\n",
    "            # If player is dead break\n",
    "            if done or r>500: #We set a max reward\n",
    "                tot_rewards.append(r)\n",
    "                r=0\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return tot_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent():\n",
    "    def agent_init(self, agent_init_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "        {\n",
    "            num_states (int): The number of states,\n",
    "            num_actions (int): The number of actions,\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor,\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.num_states = agent_init_info[\"num_states\"]\n",
    "        self.epsilon = agent_init_info[\"epsilon\"]\n",
    "        self.epsilon_end = agent_init_info[\"epsilon_end\"]\n",
    "        self.step_size = agent_init_info[\"step_size\"]\n",
    "        self.discount = agent_init_info[\"discount\"]\n",
    "        self.rand_generator = np.random.RandomState(agent_init_info[\"seed\"])\n",
    "\n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.current_eps = self.epsilon\n",
    "\n",
    "    def update_eps(self, iteration, max_iteration):\n",
    "        self.current_eps = self.epsilon #* (1 - iteration/max_iteration) + self.epsilon_end * (iteration/max_iteration)\n",
    "\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state, training = True):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "\n",
    "        if training:\n",
    "            probs = get_probs(self.q[state], self.epsilon, self.num_actions)\n",
    "            \n",
    "            expected_q = sum([probs[a] * self.q[state][a] for a in range(self.num_actions)])\n",
    "            \n",
    "            self.q[self.prev_state][self.prev_action] = self.q[self.prev_state][self.prev_action] \\\n",
    "                + self.step_size * (reward + self.discount * expected_q - self.q[self.prev_state][self.prev_action])\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "\n",
    "        self.q[self.prev_state][self.prev_action] = self.q[self.prev_state][self.prev_action] \\\n",
    "            + self.step_size * (reward  - self.q[self.prev_state][self.prev_action])\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes,agent_init_infos, plot=False):\n",
    "    agent = SarsaAgent()\n",
    "    agent.agent_init(agent_init_infos)\n",
    "    performances = {}\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        agent.update_eps(i_episode, num_episodes)\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            tot_rewards = evaluate_SARSA(env, agent)\n",
    "            performances[str(i_episode)] = tot_rewards\n",
    "            sys.stdout.flush()\n",
    "        if plot == True and i_episode in [1000,num_episodes//2,num_episodes]:\n",
    "            plot_q_function(agent.q)\n",
    "                \n",
    "        state,_ = env.reset()\n",
    "        first_action = agent.agent_start(state)\n",
    "        state, reward, done, _, info = env.step(first_action)\n",
    "        r = reward\n",
    "        while True:\n",
    "            next_action = agent.agent_step(reward,state, training = True)\n",
    "            state, reward, done, _, info = env.step(next_action)\n",
    "            # If player is dead break\n",
    "            r+=reward\n",
    "            if done or r>500:\n",
    "                agent.agent_end(reward)\n",
    "                break\n",
    "        \n",
    "        \n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    return agent, performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating for eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [0.01,0.05,0.1,0.2]:\n",
    "    print(\"Eps=\", eps)\n",
    "    agent, performances = sarsa(env,30000,{\n",
    "        \"num_actions\": env.action_space.n,\n",
    "        \"num_states\": env.observation_space[0].n * env.observation_space[1].n,\n",
    "        \"epsilon\": eps,\n",
    "        \"epsilon_end\": 0,\n",
    "        \"step_size\": 0.1,\n",
    "        \"discount\": 1.0,\n",
    "        \"seed\" : 0, \n",
    "\n",
    "    })\n",
    "    training_steps = [int(k) for k in performances.keys()]\n",
    "    mean_rewards = []\n",
    "    std_rewards = []\n",
    "    for rewards in performances.values():\n",
    "        mean_rewards.append(np.array(rewards).mean())\n",
    "        std_rewards.append(np.array(rewards).std())\n",
    "\n",
    "    plt.plot(training_steps, mean_rewards, label = \"Eps=\" +str(eps))\n",
    "plt.legend()\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Alpha (step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in [0.05,0.1,0.2,0.3]:\n",
    "    print(\"Step size=\",step)\n",
    "    agent, performances = sarsa(env,30000,{\n",
    "        \"num_actions\": env.action_space.n,\n",
    "        \"num_states\": env.observation_space[0].n * env.observation_space[1].n,\n",
    "        \"epsilon\": 0.01,\n",
    "        \"epsilon_end\": 0,\n",
    "        \"step_size\": step,\n",
    "        \"discount\": 1.0,\n",
    "        \"seed\" : 0, \n",
    "\n",
    "    })\n",
    "    training_steps = [int(k) for k in performances.keys()]\n",
    "    mean_rewards = []\n",
    "    std_rewards = []\n",
    "    for rewards in performances.values():\n",
    "        mean_rewards.append(np.array(rewards).mean())\n",
    "        std_rewards.append(np.array(rewards).std())\n",
    "\n",
    "    plt.plot(training_steps, mean_rewards, label = \"Step=\" +str(step))\n",
    "plt.legend()\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sarsa_agent, sarsa_best_performances = sarsa(env,30000,{\n",
    "        \"num_actions\": env.action_space.n,\n",
    "        \"num_states\": env.observation_space[0].n * env.observation_space[1].n,\n",
    "        \"epsilon\": 0.01,\n",
    "        \"epsilon_end\": 0,\n",
    "        \"step_size\": 0.3,\n",
    "        \"discount\": 1.0,\n",
    "        \"seed\" : 0, \n",
    "\n",
    "    }, plot = True)\n",
    "training_steps = [int(k) for k in sarsa_best_performances.keys()]\n",
    "mean_rewards = []\n",
    "std_rewards = []\n",
    "for rewards in sarsa_best_performances.values():\n",
    "    mean_rewards.append(np.array(rewards).mean())\n",
    "    std_rewards.append(np.array(rewards).std())\n",
    "\n",
    "plt.plot(training_steps, mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_performances(training_perfs_list, labels):\n",
    "    for training_perfs,label in zip(training_perfs_list,labels):\n",
    "        training_steps = [int(k) for k in training_perfs.keys()]\n",
    "        mean_rewards = []\n",
    "        std_rewards = []\n",
    "        for rewards in training_perfs.values():\n",
    "            mean_rewards.append(np.array(rewards).mean())\n",
    "            std_rewards.append(np.array(rewards).std())\n",
    "        \n",
    "        plt.plot(training_steps, mean_rewards, label =label)\n",
    "        \n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing best MCC and best SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_performances([sarsa_best_performances, mcc_best_performances], [\"SARSA\", \"MCC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to test how our best models behave on different environment sizes (height, width, and pipe holes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gaps = [1,2,3,4,5,6]\n",
    "env_hight = [9,15,20,30]\n",
    "env_width = [12,20,30,50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the screen size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sarsa = []\n",
    "scores_mcc = []\n",
    "config = []\n",
    "for h,w in zip(env_hight,env_width):\n",
    "    print(h,w)\n",
    "    env_hw = gym.make('TextFlappyBird-v0', height = h, width = w, pipe_gap = 4)\n",
    "    scores_sarsa.append(np.mean(evaluate_SARSA(env_hw, best_sarsa_agent)))\n",
    "    scores_mcc.append(np.mean(evaluate_MCC(env_hw,best_mcc_policy)))\n",
    "    config.append(str((h,w)))\n",
    "\n",
    "plt.plot(config, scores_sarsa, label = \"SARSA\", marker=\"o\")\n",
    "plt.plot(config, scores_mcc, label = \"MCC\", marker=\"o\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the pipe gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sarsa = []\n",
    "scores_mcc = []\n",
    "config = []\n",
    "for p_g in pipe_gaps:\n",
    "    env_pg = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = p_g)\n",
    "    scores_sarsa.append(np.mean(evaluate_SARSA(env_pg, best_sarsa_agent)))\n",
    "    scores_mcc.append(np.mean(evaluate_MCC(env_pg,best_mcc_policy)))\n",
    "    config.append(p_g)\n",
    "\n",
    "plt.plot(config, scores_sarsa, label = \"SARSA\", marker=\"o\")\n",
    "plt.plot(config, scores_mcc, label = \"MCC\", marker=\"o\")\n",
    "plt.grid(\"on\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
